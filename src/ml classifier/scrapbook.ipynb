{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from time import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tx: [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4]\n",
      "ty: [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "5 splits\n",
      "\n",
      "Stratified :: \n",
      "This cross-validation object is a variation of KFold that returns stratified folds.\n",
      "The folds are made by preserving the --percentage of samples for each class--.\n",
      "\n",
      "=== KFold ===\n",
      "In KFolds, each test set should not overlap, even with shuffle.\n",
      "With KFolds and shuffle, the data is shuffled once at the start, \n",
      "and then divided into the number of desired splits. \n",
      "The test data is always one of the splits, the train data is the rest.\n",
      "\n",
      "TRAIN: [-5 -3 -2 -1  0  2  3  4] TEST: [-4  1]\n",
      "TRAIN: [-5 -4 -3 -2  0  1  2  3] TEST: [-1  4]\n",
      "TRAIN: [-5 -4 -2 -1  0  1  3  4] TEST: [-3  2]\n",
      "TRAIN: [-4 -3 -2 -1  1  2  3  4] TEST: [-5  0]\n",
      "TRAIN: [-5 -4 -3 -1  0  1  2  4] TEST: [-2  3]\n",
      "\n",
      "=== Shuffle Split ===\n",
      "In ShuffleSplit, the data is shuffled every time, and then split.\n",
      "This means the test sets may overlap between the splits.\n",
      "\n",
      "TRAIN: [ 3 -1 -4 -5  1  0  2 -3] TEST: [-2  4]\n",
      "TRAIN: [ 2 -5 -2  4 -1  0 -4  1] TEST: [ 3 -3]\n",
      "TRAIN: [-4 -3  0  1 -1  3  4 -5] TEST: [-2  2]\n",
      "TRAIN: [-1  1  2  3 -2  0 -4 -3] TEST: [ 4 -5]\n",
      "TRAIN: [ 2 -3  1  0 -1 -2 -5  4] TEST: [-4  3]\n",
      "Note the overlap of the elements in the test sets for ShuffleSplit.\n"
     ]
    }
   ],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
    "# https://stackoverflow.com/questions/45969390/\n",
    "# https://www.kaggle.com/ogrellier/kfold-or-stratifiedkfold\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn import datasets\n",
    "\n",
    "# ===============\n",
    "splits = 5\n",
    "\n",
    "tx = list(range(-splits,splits))\n",
    "ty = [0] * 5 + [1] * 5\n",
    "\n",
    "print('tx:',tx)\n",
    "print('ty:',ty)\n",
    "print(splits, 'splits')\n",
    "# ===============\n",
    "\n",
    "print('''\\nStratified :: \n",
    "This cross-validation object is a variation of KFold that returns stratified folds.\n",
    "The folds are made by preserving the --percentage of samples for each class--.''')\n",
    "\n",
    "print(\"\\n=== KFold ===\")\n",
    "print('''In KFolds, each test set should not overlap, even with shuffle.\n",
    "With KFolds and shuffle, the data is shuffled once at the start, \n",
    "and then divided into the number of desired splits. \n",
    "The test data is always one of the splits, the train data is the rest.\\n''')\n",
    "kfold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kfold.split(tx, ty):\n",
    "    print(\"TRAIN:\", train_index-splits, \"TEST:\", test_index-splits)\n",
    "\n",
    "print(\"\\n=== Shuffle Split ===\")\n",
    "print('''In ShuffleSplit, the data is shuffled every time, and then split.\n",
    "This means the test sets may overlap between the splits.\\n''')\n",
    "shufflesplit = StratifiedShuffleSplit(n_splits=splits, random_state=42, test_size=2)\n",
    "for train_index, test_index in shufflesplit.split(tx, ty):\n",
    "    print(\"TRAIN:\", train_index-splits, \"TEST:\", test_index-splits)\n",
    "print('Note the overlap of the elements in the test sets for ShuffleSplit.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalizeDict(d):\n",
    "    total = sum(d.values(), 0.0)\n",
    "    for key in d:\n",
    "        d[key] /= total\n",
    "\n",
    "\n",
    "target_tokens = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## email keywords : from 'enron sent' and common english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded df common words: (100, 3)\n",
      "processsing ernon sent for common words...\n",
      "enronsent00  enronsent01  enronsent02  enronsent03  enronsent04  enronsent05  enronsent06  enronsent07  enronsent08  enronsent09  enronsent10  enronsent11  enronsent12  enronsent13  enronsent14  enronsent15  enronsent16  enronsent17  enronsent18  enronsent19  enronsent20  enronsent21  enronsent22  enronsent23  enronsent24  enronsent25  enronsent26  enronsent27  enronsent28  enronsent29  enronsent30  enronsent31  enronsent32  enronsent33  enronsent34  enronsent35  enronsent36  enronsent37  enronsent38  enronsent39  enronsent40  enronsent41  enronsent42  enronsent43  enronsent44  # unique words: 427686\n",
      "mergeUniqueInorder sanity check:  True\n",
      "['the', 'be', 'to', 'and', 'of', 'a', 'i', 'in', 'that', 'you', 'have', 'for', 'I', 'is', 'it', 'on', 'not', 'this', 'we', 'with', 'he', 'as', 'will', 'do', '>', 'at', 'are', 'but', 'if', 'his', 'by', 'from', 'or', 'they', 'your', 'say', '-', 'her', 'please', 'she', 'me', 'an', 'would', 'my', 'any', 'one', 'can', 'all', 'was', 'am', 'there', 'their', 'our', 'what', 'so', 'up', 'has', 'out', 'know', 'about', 'who', 'get', 'which', 'go', 'when', 'need', 'make', 'let', 'like', 'pm', 'time', 'no', 'should', 'just', 'him', 'may', 'take', 'people', 'been', 'into', 'year', 'new', 'good', 'some', 'could', 'call', 'them', 'gas', 'see', 'these', 'other', 'power', 'than', 'then', 'now', 'thanks', 'look', '=20', 'only', 'come', 'its', 'energy', 'over', 'think', 'more', 'also', 'back', 'after', 'attached', 'use', 'us', 'two', 'had', 'how', 'work', 'first', 'were', 'well', 'going', 'way', 'even', 'want', \"i'm\", 'because', 'information', 'thanks,', 'give', 'last', 'day', \"don't\", 'most', '&', 'meeting', 'deal', 'next', '>>', 'send', 'agreement', 'very', 'email', '=', 'thanks.', 'market', 'said', 'did', 'here', 'still', 'company', 'business', 'you.', 'forward', 'price', 'thank', 'trading', 'through', 'under', 'message', 'each', 'jeff', 'contact', 'sent', 'e-mail', 'sure', 'contract', 'following', 'group', 'credit', \"i'll\", 'before', '--', 'since', 'much', 'does', 'those', 'number', 'received', 'review', 'mark', '2001', 'file:', 'california', 'where', 'hope', 'it.', 'find', 'being', 'working', 'such', 'per', 'week', 'm', 'help', 'provide', 'john', 'state', 'both', '?', 'copy', 'office', 'set', 'until', \"it's\", 'between', 'financial', 'made', 'list', 'intended', 'got', 'legal', 'off', 'regarding', 'change', 'might', 'discuss', 'risk', 'draft', 'talk', 'put', 'houston', 'able', 'right', 'questions', 'services', 'same', 'ena', 'message-----', 'down', 'order', 'best', '-----original', '2', 'really', 'changes', 'today', '<<', 'few', 'keep', 'available', 'comments', 'issues', 'kay', 'access', 'believe', 'plan', 'me.', 'great', 'asked', 'during', 'many', 'letter', 'current', 'name', 'getting', 'vince', 'sara', 'meet', 'conference', 'date', 'mike', 'deals', 'around', 'free', 'report', 'regards,', 'rate', 'anything', 'however,', 'management', 'another', '1', 'wanted', 'system', 'due', 'request', 'phone', 'end', 'team', 'form', \"i've\", 'service', 'fax', 'looking', 'while', 'check', 'schedule', '2000', 'master', 'already', 'thought', 'must', 'contracts', 'feel', 'long', 'also,', 'below', 'issue', 'try', 'probably', 'know.', 'additional', '*', 'part', 'bill', 'based', 'corp.', 'week.', 'address', 'whether', 'little', 'project', 'jim', 'process', 'data', 'shall', 'something', 'hi', 'customers', '|', 'place', '3', '.', 'october', 'prices', 'doing', 'electricity', 'inc.', \"didn't\", 'buy', '(713)', 'pay', 'things', 'done', 'currently', 'days', 'receive', 'told', 'chris', 'questions.', 'support', 'capacity', 'needs', 'further', 'soon', 'note', 'someone', 'include', 'trying', 'given', \"can't\", 'having', 'either', 'cost', 'point', 'too', 'including', 'you,', 'ferc', 'x', '1.', 'used', 'better', 'texas', 'within', 'friday', 'trade', 'interest', 'without', '25', 'why', 'time.', 'purchase', 'continue', 'looks', \"enron's\", 'natural', 'start', 'offer', 'open', 'tomorrow', 'left', '2.', 'today.', 'confidential', 'several', 'amount', 'well.', 'section', 'steve', 'tell', 'party', 'interested', 'three', 'final', 'language', 'person', 'home', 'sorry', 'called', 'companies', 'million', 'together', 'value', 'term', 'morning', 'position', 'this.', 'move', 'anyone', 'confirm', 'daily', 'isda', 'ask', 'public', 'global', 'guys', 'total', 'month', 'monday', 'prior', 'customer', 'once', 'using', 'general', '4', 'proposed', 'david', 'possible', 'file', 'fax:', 'best,', 'big', 'case', 'understand', 'san', 'commission', 'mr.', 'firm', '>>>', 'transaction', 'sell', 'book', 'again', 'direct', 'scheduled', 'june', 'copies', 'add', 'march', 'product', 'heard', 'hear', 'never', 'wholesale', 'above', '5', 'counterparty', 'happy', 'utilities', 'west', 'taking', 'documents', 'click', 'electric', 'houston,', 'notice', 'everyone', 'every', 'short', 'november', 'and/or', '10', 'cash', 'april', 'run', 'stock', 'coming', 'return', 'july', 'delivery', 'document', 'early', 'years', 'high', 'love', 'transmission', '(or', 'option', 'agreement.', 'delete', \"you're\", 'against', 'demand', 'transactions', 'maybe', 'version', 'markets', 'line', 'products', 'pipeline', 'appreciate', 'carol']\n",
      "total natural language tokens:  501\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 3; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7373f467a0f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;31m# add to target tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m \u001b[0mtarget_tokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memail_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nupdated to complete dataset target tokens'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: dictionary update sequence element #0 has length 3; 2 is required"
     ]
    }
   ],
   "source": [
    "\n",
    "# -- token set : emails --\n",
    "\n",
    "# NL (natural language) token set is build from: \n",
    "#  english commons and the 'enron-sent' ds commons.\n",
    "\n",
    "# * 'Enron sent' is a special cleaned nl-courups of the enron corporate emails dataset\n",
    "# * Styler, Will (2011). The EnronSent Corpus. Technical Report 01-2011, \n",
    "# * University of Colorado at Boulder Institute of Cognitive Science, Boulder, CO.\n",
    "\n",
    "\n",
    "# english common\n",
    "df_cmn = pd.read_csv('email_ds/common_english/eng_top_100_common.csv')\n",
    "print('loaded df common words: {}'.format(df_cmn.shape))\n",
    "#print(df_cmn.keys())\n",
    "df_cmn['WORD'].str.lower()\n",
    "eng_cmb_list = df_cmn['WORD'].tolist()\n",
    "#print(eng_cmb_list,'\\n\\n')\n",
    "\n",
    "\n",
    "# ernon sent email commons\n",
    "\n",
    "N_TOP_WORDS_ERNON = 500\n",
    "\n",
    "def processErnonCommon():\n",
    "    \n",
    "    print('processsing ernon sent for common words...')\n",
    "    wordfreqs = {}\n",
    "    ignorelist= ['Enron', 'enron', ]\n",
    "    \n",
    "    for i in range(44+1):\n",
    "        filename = 'enronsent' + str(i).zfill(2)\n",
    "        \n",
    "        with open('email_ds/ernon/enronsent/'+filename) as text:\n",
    "            print(filename,' ', end='')\n",
    "            for line in text.readlines():\n",
    "                for word in line.split():\n",
    "                    word = word.lower()\n",
    "                    if word not in ignorelist:\n",
    "                        if word in wordfreqs:\n",
    "                            wordfreqs[word] = wordfreqs[word] + 1;\n",
    "                        else:\n",
    "                            wordfreqs[word]=1;\n",
    "\n",
    "    print('# unique words:',len(wordfreqs))\n",
    "    wf_sorted = sorted(wordfreqs.items(), key=lambda x: x[1], reverse=True)\n",
    "    #print(wf_sorted[:N_TOP_WORDS_ERNON],'\\n\\n')\n",
    "    \n",
    "    list = [x[0] for x in wf_sorted[:N_TOP_WORDS_ERNON]]\n",
    "    #print(list)\n",
    "    return list\n",
    "\n",
    "ernon_cmn_list = processErnonCommon()\n",
    "\n",
    "\n",
    "# merge common english and common 'ernon sent'\n",
    "# merge keeps order of decending frequency, \n",
    "# and outputs only unique values\n",
    "\n",
    "def mergeUniqueInorder(a,b):\n",
    "    def addinorder(a,i,c,s):\n",
    "        if a[i] not in s:\n",
    "            #print('unq:',a[i],s)\n",
    "            s.add(a[i])\n",
    "            c.append(a[i])\n",
    "    s = set()\n",
    "    c = []\n",
    "    # a shorter\n",
    "    if len(a)>len(b):\n",
    "        b, a = a, b\n",
    "    for i in range(len(a)):\n",
    "        addinorder(a,i,c,s)\n",
    "        addinorder(b,i,c,s)\n",
    "    for k in range(i+1 ,len(b)):\n",
    "        addinorder(b,k,c,s)\n",
    "    print('mergeUniqueInorder sanity check: ',len(c)==len(s))\n",
    "    return c\n",
    "\n",
    "# combine english commons and 'ernon sent' commons\n",
    "email_tokens = mergeUniqueInorder(eng_cmb_list, ernon_cmn_list)\n",
    "print(email_tokens)\n",
    "print('total natural language tokens: ',len(email_tokens))\n",
    "\n",
    "# add to target tokens\n",
    "target_tokens.update(email_tokens)\n",
    "print('\\nupdated to complete dataset target tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- codes keywords : from dedicated XML --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -- read code tokens from XML --\n",
    "\n",
    "# for C, this can be the language's keywords\n",
    "#  language-specific keywords can be maintained in a local xml\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "\n",
    "\n",
    "tree = ET.parse('src_code_ds/langs_xml.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "code_tokens = set()\n",
    "\n",
    "# setup what kind of tokens, of which languages, to take from the xml\n",
    "langs = ['C','JAVA']\n",
    "lang_tags = ['keywords', 'types', 'headers']\n",
    "\n",
    "# retrieve from XML\n",
    "for lang in root.findall('./langs/lang'):\n",
    "    name = lang.find('name').text\n",
    "    if name not in langs:\n",
    "        continue\n",
    "    print('\\n==== adding code tokens for:', name, '====')\n",
    "    for tag in lang_tags:\n",
    "        words = lang.find(tag).text\n",
    "        #print('-- adding from tag :', tag, ':', words)\n",
    "        if words:\n",
    "            code_tokens.update(words.split())\n",
    "    print('loaded {} code tokens.'.format(len(code_tokens)))\n",
    "    #print(code_tokens)\n",
    "\n",
    "    \n",
    "# conform to code dataset tokenization\n",
    "def bowConform(text): \n",
    "    pat = '<(?P<header>.+).h|c>' \n",
    "    m = re.search(pat, text)\n",
    "    if m:\n",
    "        text = m.group('header')\n",
    "        print(' {}'.format(text),end='')\n",
    "    return text\n",
    "\n",
    "\n",
    "print('\\n===> cleaning code tokens set...', end='')\n",
    "code_tokens = {bowConform(v) for v in code_tokens}\n",
    "    \n",
    "print('\\n\\n===> final code tokens set:\\n\\n',code_tokens)\n",
    "\n",
    "# add to target tokens\n",
    "target_tokens.update(code_tokens)\n",
    "print('\\n===> updated to complete-dataset target-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total target keywords:',len(target_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- tokens : term frequency-inverse document frequency (TF-IDF) --\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#display(df_codes['source'][340:350])\n",
    "#src = ast.literal_eval(src) #import ast \n",
    "\n",
    "print('text null values check:\\n====================')\n",
    "display(df['text'].isnull().value_counts())\n",
    "\n",
    "# collect selected tokens from all code corpus \n",
    "print('\\ncounting target tokens across all dataset text:\\n====================')\n",
    "global_cnt = Counter()\n",
    "tokens_col = []\n",
    "entry_cnt = Counter()\n",
    "i = 0\n",
    "print('total entries processed: ')\n",
    "for text in df['text']:\n",
    "    # loop control\n",
    "    i=i+1\n",
    "    if i % 350 == 0:\n",
    "        pass\n",
    "        #break\n",
    "    # extract entry tokens\n",
    "    tokens = [tk for tk in text.split()]\n",
    "    tokens_list = [tk for tk in tokens if (tk and tk in target_tokens)]\n",
    "    #count entry tokens\n",
    "    entry_cnt.clear()\n",
    "    entry_cnt.update(tokens_list)\n",
    "    # update entry tokencounts column\n",
    "    #  tokens_col.append(entry_cnt.most_common(20))  # will later use 01 vectors\n",
    "    # update to global frequency counter\n",
    "    global_cnt.update(entry_cnt)\n",
    "    # report\n",
    "    if i % 10000 == 0:\n",
    "        print(i,end='... ')\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- decide most common and final code tokens --\n",
    "\n",
    "# display\n",
    "\n",
    "#print(global_cnt)\n",
    "\n",
    "# counter to pd\n",
    "c = pd.Series(global_cnt, name='counts')\n",
    "c = c.sort_values(ascending=False)\n",
    "\n",
    "# box plot to find outliers\n",
    "fig = plt.figure(figsize=(16, 5), dpi=200), plt.rc(\"font\", size=8)\n",
    "c.plot(kind='box', vert=False, grid=True)\n",
    "plt.xticks(list(range(0,c.max()+1,100000)))\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "# bar plot\n",
    "print('Global target tokens frequencies:')\n",
    "fig = plt.figure(figsize=(16, 5), dpi=200), plt.rc(\"font\", size=8)\n",
    "\n",
    "ax = c.plot(kind='bar')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "# -- CODE BIN --\n",
    "\n",
    "# as dataframe - not used\n",
    "# c = pd.DataFrame(list(d.items()), columns=['token','count'])\n",
    "# c = c.sort_values(by='count', ascending=False)\n",
    "# c.set_index('token', inplace=True)\n",
    "# ax = c['count'].plot(kind='bar')\n",
    "# plt.xticks(rotation='vertical')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# normalization and cutting\n",
    "token_freq = (c/c.sum()).sort_values(ascending=False)\n",
    "#print(token_freq)\n",
    "TARGET_TKNS_NUM_MAX = 50\n",
    "TARGET_TKNS_BAR = token_freq.mean(axis='counts'), print('mean is:',TARGET_TKNS_BAR)\n",
    "\n",
    "token_freq_common = token_freq.where(token_freq > TARGET_TKNS_BAR)\n",
    "token_freq_common = token_freq_common[:TARGET_TKNS_NUM_MAX]\n",
    "\n",
    "# bar plot\n",
    "fig = plt.figure(figsize=(16, 5), dpi=200), plt.rc(\"font\", size=8)\n",
    "ax = c.plot(kind='bar')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- draw a nice code word cloud --\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from scipy.misc import imread\n",
    "\n",
    "wc = WordCloud(background_color='white', mask=None, font_path='cs_regular.ttf', \\\n",
    "               width=1600, height=800)\n",
    "wc.generate_from_frequencies(token_freq_common)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6 * 2, 4 * 2))\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "\n",
    "# preprocessor: \n",
    "#  a callable that takes an entire document as input (as a single string), \n",
    "#  and returns a possibly transformed version of the document, still as an entire string. \n",
    "#  This can be used to remove HTML tags, lowercase the entire document, etc.\n",
    "\n",
    "# tokenizer: \n",
    "#  a callable that takes the output from the preprocessor and splits it into tokens, \n",
    "#  then returns a list of these.\n",
    "\n",
    "\n",
    "# analyzer: \n",
    "#  a callable that replaces the preprocessor and tokenizer. \n",
    "#  The default analyzers all call the preprocessor and tokenizer, \n",
    "#  but custom analyzers will skip this. \n",
    "#  N-gram extraction and stop word filtering take place at the analyzer level, \n",
    "#  so a custom analyzer may have to reproduce these steps.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
